{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70346c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cf5a007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\archa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\archa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using the latest cached version of the dataset since Whispering-GPT/lex-fridman-podcast couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\archa\\.cache\\huggingface\\datasets\\Whispering-GPT___lex-fridman-podcast\\default\\0.0.0\\89ae90cf6e8d21e4f81b581252f1c8f4964b2de3 (last modified on Mon Jun  2 20:34:28 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset loaded with 346 rows and columns: ['id', 'channel', 'channel_id', 'title', 'categories', 'tags', 'description', 'text', 'segments']\n",
      "âœ… Saved dataset to data/raw/lex_fridman_podcast.csv\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Load the dataset from Hugging Face (fallbacks to cached if offline)\n",
    "dataset = load_dataset(\"Whispering-GPT/lex-fridman-podcast\")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Display structure\n",
    "print(f\"âœ… Dataset loaded with {len(df)} rows and columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Create raw data directory\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "\n",
    "# Save raw CSV\n",
    "csv_path = \"data/raw/lex_fridman_podcast.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"âœ… Saved dataset to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "766d25fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clean & Normalize ===\n",
    "df['text'] = df['text'].fillna('')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    punct = set(string.punctuation)\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words and t not in punct]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be99d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Tokenize ===\n",
    "custom_stopwords = set(stopwords.words(\"english\")).union({'s', 't', 're', 've', 'll', 'm'})\n",
    "\n",
    "def tokenize(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    return [t for t in tokens if t not in custom_stopwords and t not in string.punctuation]\n",
    "\n",
    "df['tokens'] = df['text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a05161e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cleaned dataset saved to: data/processed/lex_fridman_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# === Save Processed File ===\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "df.to_csv(\"data/processed/lex_fridman_cleaned.csv\", index=False)\n",
    "print(\"âœ… Cleaned dataset saved to: data/processed/lex_fridman_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69abcebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Lexical Summary:\n",
      "{'Total Tokens': 3473801, 'Unique Tokens': 50018, 'Lexical Diversity': 0.0144, 'Average Token Length': 5.95}\n"
     ]
    }
   ],
   "source": [
    "# === Lexical Stats ===\n",
    "all_tokens = [t for sublist in df['tokens'] for t in sublist]\n",
    "total_tokens = len(all_tokens)\n",
    "unique_tokens = len(set(all_tokens))\n",
    "lexical_diversity = unique_tokens / total_tokens if total_tokens else 0\n",
    "avg_len = sum(len(t) for t in all_tokens) / total_tokens if total_tokens else 0\n",
    "\n",
    "print(\"\\nðŸ“Š Lexical Summary:\")\n",
    "print({\n",
    "    \"Total Tokens\": total_tokens,\n",
    "    \"Unique Tokens\": unique_tokens,\n",
    "    \"Lexical Diversity\": round(lexical_diversity, 4),\n",
    "    \"Average Token Length\": round(avg_len, 2)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f717c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Guest Extraction ===\n",
    "def extract_guest_name(title):\n",
    "    if \":\" in title:\n",
    "        words = title.split(\":\")[0].split()\n",
    "        return \" \".join(words[-2:]) if len(words) >= 2 else title\n",
    "    return title\n",
    "\n",
    "df['guest'] = df['title'].apply(extract_guest_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c17e113b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title            guest\n",
      "0  Jed Buchwald: Isaac Newton and the Philosophy ...     Jed Buchwald\n",
      "1  Sergey Nazarov: Chainlink, Smart Contracts, an...   Sergey Nazarov\n",
      "2  Stephen Wolfram: Fundamental Theory of Physics...  Stephen Wolfram\n",
      "3  Philip Goff: Consciousness, Panpsychism, and t...      Philip Goff\n",
      "4  Oriol Vinyals: DeepMind AlphaStar, StarCraft, ...    Oriol Vinyals\n",
      "5  Ray Dalio: Principles, the Economic Machine, A...        Ray Dalio\n",
      "6  Michael Malice: Totalitarianism and Anarchy | ...   Michael Malice\n",
      "7  Tomaso Poggio: Brains, Minds, and Machines | L...    Tomaso Poggio\n",
      "8  George Hotz: Comma.ai, OpenPilot, and Autonomo...      George Hotz\n",
      "9  Tim Dillon: Comedy, Power, Conspiracy Theories...       Tim Dillon\n"
     ]
    }
   ],
   "source": [
    "# Print a few guest names to verify\n",
    "print(df[['title', 'guest']].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lexprojectenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
